"""
Production-ready patterns and advanced usage examples
"""
from typing import List, Dict, Any

import sys
import os
# Add parent directory to path to import aisert
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from aisert import Aisert, AisertConfig, AisertError

# Example 1: Testing Framework Integration
def test_llm_response_quality():
    """Integration with testing frameworks like pytest"""
    def validate_llm_output(response: str, expected_keywords: List[str]) -> bool:
        try:
            result = (
                Aisert(response)
                .assert_contains(expected_keywords, strict=True)
                .assert_tokens(500, strict=True)
                .collect()
            )
            return result.status
        except AisertError:
            return False
    
    # Usage in tests
    assert validate_llm_output("Python is great for data science", ["Python", "data"])
    print("✅ Test framework integration works")

# Example 2: CI/CD Pipeline Validation
def cicd_validation_pipeline():
    """Validate LLM responses in CI/CD pipelines"""
    responses_to_validate = [
        "Generated documentation for the API endpoint",
        "Code review comments generated by AI",
        "Automated test descriptions"
    ]
    
    config = AisertConfig(
        token_model="gpt-3.5-turbo",
        model_provider="openai"
    )
    
    failed_validations = []
    
    for i, response in enumerate(responses_to_validate):
        try:
            result = (
                Aisert(response, config)
                .assert_tokens(200, strict=False)
                .assert_contains(["generated", "AI", "automated"], strict=False)
                .collect()
            )
            if not result.status:
                failed_validations.append(f"Response {i+1}")
        except Exception as e:
            failed_validations.append(f"Response {i+1}: {e}")
    
    if failed_validations:
        print(f"❌ CI/CD Pipeline failed: {failed_validations}")
    else:
        print("✅ CI/CD Pipeline validation passed")

# Example 3: A/B Testing LLM Responses
def ab_test_llm_responses():
    """Compare different LLM responses for quality"""
    response_a = "Our product helps businesses streamline operations and increase efficiency."
    response_b = "This solution optimizes workflows and boosts productivity for companies."
    
    def score_response(response: str) -> Dict[str, Any]:
        result = (
            Aisert(response)
            .assert_contains(["business", "efficiency", "productivity"], strict=False)
            .assert_tokens(100, strict=False)
            .assert_semantic_matches("business productivity solution", 0.6, strict=False)
            .collect()
        )
        
        return {
            "response": response,
            "overall_score": result.status,
            "details": result.rules
        }
    
    score_a = score_response(response_a)
    score_b = score_response(response_b)
    
    print(f"Response A Score: {score_a['overall_score']}")
    print(f"Response B Score: {score_b['overall_score']}")

# Example 4: Content Quality Monitoring
class ContentQualityMonitor:
    """Monitor LLM-generated content quality in production"""
    
    def __init__(self):
        self.config = AisertConfig(
            token_model="gpt-3.5-turbo",
            model_provider="openai"
        )
        self.quality_metrics = {
            "total_responses": 0,
            "passed_validation": 0,
            "failed_validation": 0
        }
    
    def validate_content(self, content: str, requirements: Dict[str, Any]) -> bool:
        """Validate content against dynamic requirements"""
        self.quality_metrics["total_responses"] += 1
        
        try:
            aisert = Aisert(content, self.config)
            
            # Dynamic validation based on requirements
            if "keywords" in requirements:
                aisert.assert_contains(requirements["keywords"], strict=False)
            
            if "max_tokens" in requirements:
                aisert.assert_tokens(requirements["max_tokens"], strict=False)
            
            if "semantic_match" in requirements:
                aisert.assert_semantic_matches(
                    requirements["semantic_match"]["text"],
                    requirements["semantic_match"]["threshold"],
                    strict=False
                )
            
            result = aisert.collect()
            
            if result.status:
                self.quality_metrics["passed_validation"] += 1
                return True
            else:
                self.quality_metrics["failed_validation"] += 1
                return False
                
        except Exception as e:
            self.quality_metrics["failed_validation"] += 1
            print(f"Validation error: {e}")
            return False
    
    def get_quality_report(self) -> Dict[str, Any]:
        """Get quality metrics report"""
        total = self.quality_metrics["total_responses"]
        if total == 0:
            return {"quality_score": 0, "metrics": self.quality_metrics}
        
        quality_score = self.quality_metrics["passed_validation"] / total
        return {
            "quality_score": quality_score,
            "metrics": self.quality_metrics
        }

# Example 5: Custom Validation Rules
def custom_validation_example():
    """Demonstrate extensible validation patterns"""
    
    def validate_customer_service_response(response: str) -> bool:
        """Custom validation for customer service responses"""
        requirements = {
            "keywords": ["thank", "help", "assist"],
            "max_tokens": 150,
            "semantic_match": {
                "text": "helpful customer service response",
                "threshold": 0.7
            }
        }
        
        monitor = ContentQualityMonitor()
        return monitor.validate_content(response, requirements)
    
    # Test responses
    good_response = "Thank you for contacting us. I'm here to help and assist you with your inquiry."
    bad_response = "I don't know. Figure it out yourself."
    
    print(f"Good response valid: {validate_customer_service_response(good_response)}")
    print(f"Bad response valid: {validate_customer_service_response(bad_response)}")

# Example 6: Performance Monitoring
def performance_monitoring():
    """Monitor validation performance in production"""
    import time
    
    responses = [
        "Sample response 1 for performance testing",
        "Sample response 2 for performance testing", 
        "Sample response 3 for performance testing"
    ] * 10  # 30 responses total
    
    config = AisertConfig(
        token_model="gpt-3.5-turbo",
        model_provider="openai"
    )
    
    start_time = time.time()
    
    for response in responses:
        result = (
            Aisert(response, config)
            .assert_contains(["response", "testing"], strict=False)
            .assert_tokens(100, strict=False)
            .collect()
        )
    
    end_time = time.time()
    
    print(f"Processed {len(responses)} responses in {end_time - start_time:.2f} seconds")
    print(f"Average time per validation: {(end_time - start_time) / len(responses):.3f} seconds")

if __name__ == "__main__":
    print("=== Production Patterns Examples ===\n")
    
    print("1. Testing Framework Integration:")
    test_llm_response_quality()
    
    print("\n2. CI/CD Pipeline Validation:")
    cicd_validation_pipeline()
    
    print("\n3. A/B Testing:")
    ab_test_llm_responses()
    
    print("\n4. Custom Validation Rules:")
    custom_validation_example()
    
    print("\n5. Performance Monitoring:")
    performance_monitoring()